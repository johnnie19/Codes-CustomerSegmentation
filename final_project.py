# -*- coding: utf-8 -*-
"""Final Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Sasnz0SU8zX0MhGCbTpV-m9YS022pcdF
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
from numpy import percentile
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from datetime import datetime
# %matplotlib inline
from sklearn import preprocessing
from sklearn.cluster import KMeans,DBSCAN
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to compute distances
from scipy.spatial.distance import pdist
from scipy.spatial.distance import cdist

# to perform hierarchical clustering, compute cophenetic correlation, and create dendrograms
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage, cophenet

# Import the necessary libraries
from google.colab import drive
import pandas as pd

# Mount the Google Drive
drive.mount('/content/drive')

# Get the file ID
file_id = '1j5YfvXmhWMmjAp9v8g1Eo_azqQdg7LNH'

# Read the file
sampled_df = pd.read_csv('/content/drive/MyDrive/transaction.csv')

# Print the DataFrame
print(sampled_df)



df=sampled_df

df.describe()

df.head()

def check(df):
    l=[]
    columns=df.columns
    for col in columns:
        dtypes=df[col].dtypes
        nunique=df[col].nunique()
        sum_null=df[col].isnull().sum()
        l.append([col,dtypes,nunique,sum_null])
    df_check=pd.DataFrame(l)
    df_check.columns=['column','dtypes','nunique','sum_null']
    return df_check
check(df)

df.isna().sum()

df = df.dropna()

df.isna().sum()

df.duplicated().sum()

df['CustomerDOB'].value_counts()

df = df.drop(df[df['CustomerDOB'] == '1/1/1800'].index,axis = 0)

df['CustomerDOB'].value_counts()

date_string = '26/11/96'

# Split the date string into day, month, and year components
day, month, year = date_string.split('/')

# Check if the year has two digits and add '19' or '20' as appropriate
if len(year) == 2:
    if int(year) >= 30:
        year = '19' + year
    else:
        year = '20' + year

# Construct the updated date string
updated_date_string = f'{day}/{month}/{year}'

print(updated_date_string)

df["CustomerDOB"] = pd.to_datetime(df["CustomerDOB"], dayfirst=True)
df

df['TransactionDate'] = pd.to_datetime(df['TransactionDate'], dayfirst=True)
df

df['CustomerAge'] =df['TransactionDate'].dt.year - df['CustomerDOB'].dt.year

df.columns = df.columns.str.strip()  # Clean column names
df.columns

import datetime as dt
snapshot_date = df['TransactionDate'].max() + dt.timedelta(days=1)
df_rfm = df.groupby('CustomerID').agg({
    'TransactionDate': lambda x: (snapshot_date - x.max()).days,
    'TransactionID': 'count',
    "TransactionAmount (INR)": 'sum'
})

df_rfm.rename(columns={
    'TransactionDate': 'recency',
    'TransactionID': 'frequency',
    "TransactionAmount (INR)": 'monetary_value',
}, inplace = True)

import pandas as pd

# Create bins and labels for each RFM score
r_bins = [0, 30, 60, 90, df_rfm['recency'].max()]
f_bins = [0, 1, 2, 3, df_rfm['frequency'].max()]  # Removed the last edge from f_bins
m_bins = [0, 100, 500, 1000, df_rfm['monetary_value'].max()]
r_labels = [1, 2, 3, 4]  # Updated labels for r_score
f_labels = [4, 3, 2, 1]  # Updated labels for f_score
m_labels = [4, 3, 2, 1]  # Updated labels for m_score

r_bins.sort()
f_bins.sort()
m_bins.sort()

df_rfm['r_score'] = pd.cut(df_rfm['recency'], bins=r_bins, labels=r_labels, include_lowest=True, duplicates='drop')
df_rfm['f_score'] = pd.cut(df_rfm['frequency'], bins=f_bins, labels=f_labels, include_lowest=True, duplicates='drop')
df_rfm['m_score'] = pd.cut(df_rfm['monetary_value'], bins=m_bins, labels=m_labels, include_lowest=True, duplicates='drop')

df_rfm['RFM'] = df_rfm['r_score'].astype(str) + df_rfm['f_score'].astype(str) + df_rfm['m_score'].astype(str)

seg = {
    'Champions': [555, 554, 544, 545, 454, 455, 445],
    'Loyal': [543, 444, 435, 355, 354, 345, 344, 335],
    'Potential Loyalists': [553, 551, 552, 541, 542, 533, 532, 531, 452, 451, 442, 441, 431, 453, 433, 432, 423, 353, 352, 351, 342, 341, 333, 323],
    'New Customers': [512, 511, 422, 421, 412, 411, 311],
    'Promising': [525, 524, 523, 522, 521, 515, 514, 513, 425, 424, 413, 414, 415, 315, 314, 313],
    'Need Attention': [535, 534, 443, 434, 343, 334, 325, 324],
    'About To Sleep': [331, 321, 312, 221, 213, 231, 241, 251],
    'At Risk': [255, 254, 245, 244, 253, 252, 243, 242, 235, 234, 225, 224, 153, 152, 145, 143, 142, 135, 134, 133, 125, 124],
    'Cannot Lose Them': [155, 154, 144, 214, 215, 115, 114, 113],
    'Hibernating customers': [332, 322, 233, 232, 223, 222, 132, 123, 122, 212, 211],
    'Lost customers': [111, 112, 121, 131, 141, 151]
}

def get_segment(x):
    for segment, codes in seg.items():
        if int(x) in codes:
            return segment
    return 'Unknown'

df_rfm['segment'] = df_rfm['RFM'].apply(get_segment)

#Combine the segment column with the main dataframe
segment = df_rfm['segment']

# Merge the dataframes based on CustomerID
df = pd.merge(df, segment, on='CustomerID')

df

df['TransactionDate'].max()

df['TransactionDate'].min()

df['CustomerAge'].describe()

df['CustomerAge'].max()

# Assuming your dataset is stored in a pandas DataFrame called 'data'
# Assuming the column with negative values is called 'column_name'

# Remove rows with negative values from the specific column
filtered_df_1 = df[df['CustomerAge'] < 0]
filtered_df_2 = df[df['CustomerAge'] > 100]

indices_to_drop = filtered_df_1.index.union(filtered_df_2.index)

# Drop the resulting rows from the original DataFrame
df.drop(indices_to_drop, inplace=True)

df['CustomerAge'].describe()

df.drop(columns=['TransactionTime'], inplace=True)

df['CustGender'].value_counts()

df['CustAccountBalance'].max()

df.groupby(by='CustLocation')['CustAccountBalance'].sum().sort_values(ascending=False)

df[df['CustAccountBalance'] == 0]

df[df['TransactionAmount (INR)'] > 1000000]

df['CustLocation'].value_counts()

cities = df['CustLocation'].unique().tolist()
print(cities)
print(len(cities))

df.groupby(by='CustLocation')['CustAccountBalance'].max()

num_col = df.select_dtypes(include=np.number)
cat_col = df.select_dtypes(exclude=np.number)

plt.style.use("fivethirtyeight")
plt.figure(figsize=(30,30))
for index,column in enumerate(num_col):
    plt.subplot(7,4,index+1)
    sns.boxplot(data=num_col,x=column)

plt.tight_layout(pad = 1.0)

sns.countplot(data = df, x ='CustGender')

sns.boxplot(data=df, x='CustGender', y='CustomerAge')

sns.boxplot(data=df, x='CustGender', y='CustAccountBalance')

df['TransactionDate'].dt.month.value_counts()

df['TransactionMonth'] = df['TransactionDate'].dt.month

df.columns

df.head()

sns.lineplot(data = df , x = 'TransactionMonth' , y = 'TransactionAmount (INR)')

sns.lineplot(data = df, x='TransactionMonth', y='TransactionAmount (INR)', ci=False, hue='CustGender')

sns.lineplot(data = df , x = 'TransactionMonth' , y = 'CustAccountBalance')

sns.barplot(data=df, x='CustomerAge', y='TransactionAmount (INR)')

sns.barplot(data=df, x='CustomerAge', y='CustAccountBalance')

plt.figure(figsize=(40,7))
sns.pairplot(data = df ,hue='CustGender')

df.info()

sns.heatmap(data=df[['CustomerAge','TransactionMonth','TransactionAmount (INR)','CustAccountBalance']].corr(), annot=True);

sns.histplot(x = df['TransactionDate'].dt.month, bins = 3, binwidth = 1)
plt.title('Number of transactions in each month')

print(df.columns)

from sklearn.preprocessing import LabelEncoder

original_column_name = 'CustGender'
duplicate_column_name = 'CustGender_x'

# Check if the original column exists in the DataFrame
if original_column_name not in df.columns:
    print(f"Error: '{original_column_name}' does not exist in the DataFrame.")
else:
    # Create a LabelEncoder object
    label_encoder = LabelEncoder()

    # Fit and transform the original column values
    encoded_values = label_encoder.fit_transform(df[original_column_name])

    # Create a new column in the DataFrame with the encoded values
    df[duplicate_column_name] = encoded_values

from sklearn.preprocessing import LabelEncoder

original_column_place = 'CustLocation'
duplicate_column_place = 'CustLocation_x'

# Check if the original column exists in the DataFrame
if original_column_place not in df.columns:
    print(f"Error: '{original_column_place}' does not exist in the DataFrame.")
else:
    # Create a LabelEncoder object
    label_encoder = LabelEncoder()

    # Fit and transform the original column values
    encoded_values = label_encoder.fit_transform(df[original_column_place])

    # Create a new column in the DataFrame with the encoded values
    df[duplicate_column_place] = encoded_values

df

df.dtypes

del_col = ['TransactionID', 'CustomerDOB', 'TransactionDate']
df.drop(columns=del_col, inplace=True)

df.columns

df.head()

df.to_excel('Clean_bank_transaction.xlsx', index=False)

df.reset_index(drop=True)

df.drop(columns='CustomerID', inplace=True)

import pandas as pd

df = pd.read_excel(r"/content/Clean_bank_transaction.xlsx")

correlation_matrix = df.corr()
correlation_matrix

X = df[['CustAccountBalance', 'CustomerAge', 'CustLocation_x']]
y = df['TransactionAmount (INR)']

X.shape

y.shape

from sklearn.model_selection import train_test_split
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# Normalize the features
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Convert continuous labels to binary labels
threshold = 0.5
y_train_binary = (y_train > threshold).astype(int)
y_test_binary = (y_test > threshold).astype(int)

# fit the logistic regression model
model = LogisticRegression()
model.fit(X_train, y_train_binary)

# make predictions on the test set
y_pred_binary = model.predict(X_test)

# evaluate the model
accuracy = model.score(X_test, y_test_binary)
print('Accuracy: {:.2f}%'.format(accuracy*100))

# calculate precision
precision = np.sum(y_pred_binary == y_test_binary) / len(y_pred_binary)

# calculate recall
recall = np.sum(y_pred_binary == y_test_binary) / len(y_test_binary)

# calculate F1 score
f1_score = 2 * (precision * recall) / (precision + recall)

print('Precision: {:.2f}'.format(precision))
print('Recall: {:.2f}'.format(recall))
print('F1 score: {:.2f}'.format(f1_score))

coef = model.coef_[0]
intercept = model.intercept_

# Define the decision boundary function
def decision_boundary(x):
    return -(coef[0] * x + intercept) / coef[1]

# Generate input data for plotting
x_min = X_test[:, 0].min()
x_max = X_test[:, 0].max()
x_vals = np.linspace(x_min, x_max, 100)
y_vals = decision_boundary(x_vals)

# Plot the decision boundary
plt.plot(x_vals, y_vals, c='r', label='Decision Boundary')

# Plot the data points
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test_binary, cmap='coolwarm', edgecolors='k', label='Data Points')

# Plot the legend
plt.legend()

# Set the x and y labels
plt.xlabel('Age')
plt.ylabel('AverageTransactionAmount')

# Set the title
plt.title('Logistic Regression Decision Boundary and Sigmoid Curve')

# Show the plot
plt.show()

# Generate input data for plotting the sigmoid curve
sigmoid_vals = 1 / (1 + np.exp(-(coef[0]*x_vals + intercept)))

# Plot the sigmoid curve
plt.plot(x_vals, sigmoid_vals, c='b', label='Sigmoid Curve')

X_test

plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='coolwarm', edgecolors='k')
plt.xlabel('Age')
plt.ylabel('AverageTransactionAmount')
plt.title('Scatter Plot of Data Points')
plt.show()

import pandas as pd
import numpy as np
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Normalize the features
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Create an SVR model with a Gaussian kernel
model_S = SVR(kernel='rbf')

# Fit the model to the training data
model_S.fit(X_train, y_train)

# Make predictions on the test set
y_pred_S= model_S.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred_S)
mae = mean_absolute_error(y_test, y_pred_S)
r2 = r2_score(y_test, y_pred_S)

print('Mean Squared Error:', mse)
print('Mean Absolute Error:', mae)
print('R2 Score:', r2)

train_score_S = model_S.score(X_train, y_train)
test_score_S = model_S.score(X_test, y_test)

print("Training R-squared:", train_score_S)
print("Test R-squared:", test_score_S)

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Normalize the features
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Convert continuous labels to categorical labels
num_bins = 5  # Specify the number of bins for discretization
y_train_categorical = pd.cut(y_train, bins=num_bins, labels=False)

# Create the parameter grid
param_grid = {
    'bootstrap': [True],
    'max_depth': [80, 90, 100, 110],
    'max_features': [2, 3, 4],
    'min_samples_leaf': [3, 4, 5],
    'min_samples_split': [8, 10, 12],
    'n_estimators': [100, 200, 300, 400, 500, 1000]
}

# Create a base model
rf = RandomForestRegressor()

# Instantiate the randomized search model
random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_grid,
                                   n_iter=50, cv=StratifiedKFold(n_splits=3),
                                   n_jobs=2, verbose=2, random_state=42)

# Fit the randomized search model
random_search.fit(X_train, y_train_categorical)

# Print the best hyperparameters found
print("Best Hyperparameters:", random_search.best_params_)

print(random_search.best_params_)

model = RandomForestRegressor(n_estimators=400,
                                random_state=42,
                                oob_score=True,
                                bootstrap=True,
                                max_depth=110,
                                max_features=2,
                                min_samples_leaf=4,
                                min_samples_split=8)

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Normalize the features
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Convert continuous labels to categorical labels
num_bins = 5  # Specify the number of bins for discretization
y_train_categorical = pd.cut(y_train, bins=num_bins, labels=False)
y_test_categorical = pd.cut(y_test, bins=num_bins, labels=False)

model = RandomForestRegressor(n_estimators=400,
                                random_state=42,
                                oob_score=True,
                                bootstrap=True,
                                max_depth=110,
                                max_features=2,
                                min_samples_leaf=4,
                                min_samples_split=8)
model.fit(X_train, y_train_categorical)

# Make predictions on the test set
y_pred_categorical = model.predict(X_test)

# Calculate Mean Squared Error (MSE)
mse = mean_squared_error(y_test_categorical, y_pred_categorical)
print('Mean Squared Error (MSE):', mse)

# Calculate Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test_categorical, y_pred_categorical)
print('Mean Absolute Error (MAE):', mae)

# Calculate R-squared (R2) score
r2 = r2_score(y_test_categorical, y_pred_categorical)
print('R-squared (R2) Score:', r2)

from sklearn.tree import export_graphviz
import graphviz

for i, tree in enumerate(model.estimators_):
    dot_data = export_graphviz(tree, feature_names=['CustomerAge', 'CustLocation_x', 'CustAccountBalance'], filled=True, out_file=None)
    graph = graphviz.Source(dot_data)
    graph.render(f"random_forest_tree_{i+1}")  # Save each tree as a separate file

importances = model.feature_importances_
feature_names = ['CustomerAge', 'CustLocation_x', 'CustAccountBalance']
for feature, importance in zip(feature_names, importances):
    print(f"{feature}: {importance}")

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

# Define the hyperparameters to be tuned
param_grid = {
    'max_depth': [2, 4, 6, 8],
    'min_samples_split': [2, 4, 8],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2'],
    'criterion': ['friedman_mse', 'absolute_error', 'squared_error', 'poisson']
}

# Create a Decision Tree Regressor
regressor = DecisionTreeRegressor(random_state=42)

# Instantiate the grid search model
grid_search = GridSearchCV(estimator=regressor, param_grid=param_grid, cv=5, n_jobs=2, verbose=2)

# Fit the grid search model to the data
grid_search.fit(X_train, y_train)

# Print the best parameters
print("Best parameters:", grid_search.best_params_)

# Use the best hyperparameters to train a new model
regressor1 = DecisionTreeRegressor(max_depth=4,
                                  min_samples_split=2,
                                  min_samples_leaf=1,
                                  max_features='sqrt',
                                  criterion='poisson',
                                  random_state=42)
regressor2 = DecisionTreeRegressor(max_depth=3,
                                  min_samples_split=5,
                                  min_samples_leaf=5,
                                  max_features='sqrt',
                                  criterion='friedman_mse',
                                  random_state=42)
regressor1.fit(X_train, y_train)
regressor2.fit(X_train, y_train)

# Evaluate the model on the test set
y_pred_DT1 = regressor1.predict(X_test)
y_pred_DT2 = regressor2.predict(X_test)
mse1 = mean_squared_error(y_test, y_pred_DT1)
r21 = r2_score(y_test, y_pred_DT1)
mse2 = mean_squared_error(y_test, y_pred_DT2)
r22 = r2_score(y_test, y_pred_DT2)
print(f"Mean Squared Error 1: {mse1:.2f}")
print(f"R2 Score 1: {r21:.2f}")
print(f"Mean Squared Error 2: {mse2:.2f}")
print(f"R2 Score 2: {r22:.2f}")

train_score1 = regressor1.score(X_train, y_train)
test_score1 = regressor1.score(X_test, y_test)

train_score2 = regressor2.score(X_train, y_train)
test_score2 = regressor2.score(X_test, y_test)

print("Regressor 1 - Training R-squared:", train_score1)
print("Regressor 1 - Test R-squared:", test_score1)

print("Regressor 2 - Training R-squared:", train_score2)
print("Regressor 2 - Test R-squared:", test_score2)

X_test

y_test

# Plot the results
plt.figure()
plt.scatter(X_test.iloc[:, 1], y_test, s=20, edgecolor="black", c="darkorange", label="data")
plt.plot(X_test.iloc[:, 1], y_pred_DT1, color="cornflowerblue", label="max_depth=2", linewidth=2)
plt.plot(X_test.iloc[:, 1], y_pred_DT2, color="yellowgreen", label="max_depth=5", linewidth=2)
plt.xlabel("Age")
plt.ylabel("TransactionAmount")
plt.title("Decision Tree Regression")
plt.legend()
plt.show()

from sklearn.tree import export_graphviz
import graphviz

dot_data = export_graphviz(regressor1, out_file=None, feature_names=['Age', 'CustLocation_x', 'CustAccountBalance'], filled=True)
graph = graphviz.Source(dot_data)
graph.render("decision_tree")
graph.view()

from sklearn.tree import export_graphviz
import graphviz

dot_data = export_graphviz(regressor2, out_file=None, feature_names=['Age', 'CustLocation_x', 'CustAccountBalance'], filled=True)
graph = graphviz.Source(dot_data)
graph.render("decision_tree_2")
graph.view()

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
import numpy as np

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Create polynomial features
poly_features = PolynomialFeatures(degree=3)
X_train_poly = poly_features.fit_transform(X_train)
X_test_poly = poly_features.transform(X_test)

# Fit the polynomial regression model
poly_reg = LinearRegression()
poly_reg.fit(X_train_poly, y_train)

# Predict on the test set
y_pred_poly = poly_reg.predict(X_test_poly)

# Calculate evaluation metrics
mse = mean_squared_error(y_test, y_pred_poly)
mae = mean_absolute_error(y_test, y_pred_poly)
r2 = r2_score(y_test, y_pred_poly)

# Print the evaluation metrics
print('Mean Squared Error (MSE):', mse)
print('Mean Absolute Error (MAE):', mae)
print('R-squared (R2) Score:', r2)

poly_reg.score(X_train_poly, y_train)
poly_reg.score(X_test_poly, y_test)

import matplotlib.pyplot as plt

# Plot the training data
plt.plot(X_train_poly, y_train, 'o', label='Training data')

# Add a legend
plt.legend()

# Show the plot
plt.show()

# Plot the test data
plt.plot(X_test_poly, y_test, 'x', label='Test data')

# Plot the polynomial regression line
plt.plot(X_test, y_pred_poly, 'r-', label='Polynomial regression line')

# Add a legend
plt.legend()

# Show the plot
plt.show()



from sklearn.metrics import silhouette_score
from sklearn.model_selection import GridSearchCV
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Define parameter grid
param_grid = {
    'eps': [0.1, 0.2, 0.3, 0.4, 0.5],
    'min_samples': [5, 10, 15, 20, 25]
}

# Define custom scoring function
def dbscan_silhouette_score(estimator, X):
    labels = estimator.fit_predict(X)
    if len(set(labels)) == 1:
        return -1
    else:
        return silhouette_score(X, labels)

# Perform grid search using 5-fold cross-validation with custom scoring function
dbscan = DBSCAN()
grid_search = GridSearchCV(dbscan, param_grid=param_grid, cv=5, n_jobs=-1, scoring=dbscan_silhouette_score)
grid_search.fit(X_scaled)

# Print best hyperparameters and corresponding performance metrics
print('Best hyperparameters:', grid_search.best_params_)
print('Best mean silhouette score:', grid_search.best_score_)

# Fit the model to the standardized data
labels = dbscan.fit_predict(X_scaled)

# Print the number of clusters and their sizes
print('Number of clusters:', len(set(labels)))
print('Cluster sizes:', {i: sum(labels == i) for i in set(labels)})

# Filter out noise points (-1 label)
clustered_points = X_scaled[labels != -1]
cluster_labels = labels[labels != -1]

# Plot the results with modified palette
plt.figure(figsize=(10, 10))
sns.scatterplot(x=clustered_points[:, 1], y=df['TransactionAmount (INR)'][labels != -1], hue=cluster_labels, palette='Set1', s=50)
plt.xlabel('Age')
plt.ylabel('AverageTransactionAmount')
plt.title('DBSCAN Clustering for Customer Segmentation')
plt.legend()
plt.show()

X_train

X_scaled

!pip install scikit-learn-extra

from sklearn.metrics import silhouette_score
from sklearn.model_selection import GridSearchCV
from sklearn_extra.cluster import KMedoids
from sklearn.preprocessing import StandardScaler

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Define parameter grid
param_grid = {
    'n_clusters': [2, 3, 4, 5, 6]
}

# Define custom scoring function
def kmedoids_silhouette_score(estimator, X):
    labels = estimator.fit_predict(X)
    if len(set(labels)) == 1:
        return -1
    else:
        return silhouette_score(X, labels)

# Perform grid search using 5-fold cross-validation with custom scoring function
kmedoids = KMedoids()
grid_search = GridSearchCV(kmedoids, param_grid=param_grid, cv=5, n_jobs=-1, scoring=kmedoids_silhouette_score)
grid_search.fit(X_scaled)

# Print best hyperparameters and corresponding performance metrics
print('Best hyperparameters:', grid_search.best_params_)
print('Best mean silhouette score:', grid_search.best_score_)

# Fit the model to the standardized data
labels = grid_search.best_estimator_.fit_predict(X_scaled)

# Print the number of clusters and their sizes
print('Number of clusters:', len(set(labels)))
print('Cluster sizes:', {i: sum(labels == i) for i in set(labels)})

# Plot the results
plt.figure(figsize=(10, 10))
sns.scatterplot(x=X_scaled[:, 1], y=df['TransactionAmount (INR)'], hue=labels, palette='viridis', s=50)
plt.xlabel('Age')
plt.ylabel('AverageTransactionAmount')
plt.title('K-medoids Clustering for Customer Segmentation')
plt.legend()
plt.show()

X_test.shape

y_test

from tensorflow import keras

model = keras.Sequential([
    keras.layers.Dense(16,input_shape=(3,),activation='relu'),
    keras.layers.Dense(1,activation='sigmoid')
])

model.compile(
    optimizer='adam',
    loss = 'binary_crossentropy',
    metrics=['accuracy']
)


# Train the model
model.fit(X_train, y_train, epochs=150)

# Save the model
model.save("model.h5")

# Evaluate the model on the test dataset
loss, accuracy = model.evaluate(X_test, y_test)
print("Loss:", loss)
print("Accuracy:", accuracy)

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error

# Define the parameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.1, 0.01, 0.001]
}

# Create the regressor
regressor_GB = GradientBoostingRegressor()

# Create the GridSearchCV object
grid_search = GridSearchCV(estimator=regressor_GB, param_grid=param_grid, cv=5)

# Fit the GridSearchCV object to the data
grid_search.fit(X_train, y_train)

# Get the best parameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

# Train the regressor with the best parameters
regressor_GB = GradientBoostingRegressor(**best_params)
regressor_GB.fit(X_train, y_train)

# Predict the values for the test set
y_pred_GB = regressor_GB.predict(X_test)

# Calculate the mean squared error (MSE)
mse = mean_squared_error(y_test, y_pred_GB)

# Print the best parameters, best score, and MSE
print("Best Parameters:", best_params)
print("Best Score:", best_score)
print("Mean Squared Error (MSE):", mse)

# Fit the regressor to the data
regressor_GB.fit(X_train, y_train)

# Compute the test set deviance for each boosting iteration
test_score = np.zeros((regressor_GB.n_estimators,), dtype=np.float64)
for i, y_pred_GB in enumerate(regressor_GB.staged_predict(X_test)):
    test_score[i] = mean_squared_error(y_test, y_pred_GB)

# Plot the deviance for training and test sets against boosting iterations
plt.figure(figsize=(6, 6))
plt.title("Deviance")
plt.plot(
    np.arange(regressor_GB.n_estimators) + 1,
    regressor_GB.train_score_,
    "b-",
    label="Training Set Deviance",
)
plt.plot(
    np.arange(regressor_GB.n_estimators) + 1, test_score, "r-", label="Test Set Deviance"
)
plt.legend(loc="upper right")
plt.xlabel("Boosting Iterations")
plt.ylabel("Deviance")
plt.tight_layout()
plt.show()

import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score

# Create the XGBoost model
model_XGB = xgb.XGBRegressor()

# Train the model
model_XGB.fit(X_train, y_train)

# Predict the values for the test set
y_pred_XGB = model_XGB.predict(X_test)

# Calculate the mean squared error (MSE)
mse = mean_squared_error(y_test, y_pred_XGB)

# Calculate the R2 score
r2 = r2_score(y_test, y_pred_XGB)

# Calculate the mean tweedie deviance
score = model_XGB.score(X_test, y_test)

# Print the metrics
print("Mean Squared Error (MSE):", mse)
print("R2 Score:", r2)
print("Mean Tweedie Deviance:", score)